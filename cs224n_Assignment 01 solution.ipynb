{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPX/vetLtRNGJJE8KDZI+Hu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajibmondal/Projects.NLP/blob/master/cs224n_Assignment%2001%20solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m90n0Cte1AT3",
        "colab_type": "code",
        "outputId": "bc7af0e5-a7ba-4572-8055-899bc656435f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "! git clone https://github.com/zhanlaoban/CS224N-Stanford-Winter-2019"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CS224N-Stanford-Winter-2019'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (205/205), 361.57 MiB | 36.26 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Checking out files: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBptDw64PZG",
        "colab_type": "code",
        "outputId": "7325db46-52af-49c5-9bf5-09205dbc2912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# All Import Statements Defined Here\n",
        "# Note: Do not add to this list.\n",
        "# All the dependencies you need, can be installed by running .\n",
        "# ----------------\n",
        "\n",
        "import sys\n",
        "assert sys.version_info[0]==3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5zTPddK4P-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InXcbXYC4QJN",
        "colab_type": "code",
        "outputId": "383eeafc-8c42-4b0f-9f38-03bec68bfc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
            "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
            "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
            "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
            "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
            "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
            "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
            "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
            "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
            "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
            "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
            "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
            "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
            "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
            "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
            "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
            "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
            "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
            "  'they', 'noted', '.', '<END>'],\n",
            " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
            "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
            "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
            "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
            "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
            "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
            "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
            "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
            "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
            "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
            "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
            "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
            "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
            "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
            "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
            "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
            "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
            "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
            "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
            "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
            "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
            "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
            "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
            "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
            "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
            "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
            "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
            "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
            "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
            "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
            "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
            "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
            "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
            "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
            "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
            "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
            "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
            "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
            "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
            "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
            "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
            "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
            "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
            "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
            "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
            "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
            "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
            "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
            "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
            "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
            "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
            "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
            "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
            "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
            "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
            "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
            "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
            "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
            "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
            "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
            "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
            "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
            "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
            "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
            "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
            "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
            "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
            "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
            "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
            "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
            "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
            "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
            "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
            "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
            "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
            "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
            "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
            "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
            "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
            "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
            "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
            "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
            "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
            "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
            "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
            "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
            "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
            "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
            "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
            "  '<END>'],\n",
            " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
            "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
            "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
            "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
            "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
            "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
            "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
            "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
            "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
            "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
            "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
            "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
            "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
            "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
            "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
            "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
            "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
            "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
            "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
            "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
            "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtUu-bS-4rLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "        Return:\n",
        "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    corpus_words = [y for x in corpus for y in x]\n",
        "    corpus_words = sorted(list(set(corpus_words)))\n",
        "    num_corpus_words = len(corpus_words)    \n",
        "\n",
        "\n",
        "    # ------------------\n",
        "\n",
        "    return corpus_words, num_corpus_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZgRZ9nz4rQH",
        "colab_type": "code",
        "outputId": "2544cf7a-807a-4972-d7af-add85023c1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted(list(set([\"START\", \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", \"END\"])))\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plkAHENo4rXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
        "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (numpy matrix of shape (number of corpus words, number of number of corpus words)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2Ind = {}\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    word2Ind = dict(zip(words, range(num_words)))\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    \n",
        "    for _ , sentences in enumerate(corpus):\n",
        "        for i in range(len(sentences) - window_size +1):\n",
        "            curr_word = sentences[i]\n",
        "            if i == 0:\n",
        "                for j in range(min(window_size, len(sentences))):\n",
        "                    neighbor_word = sentences[i+j+1]\n",
        "                    M[word2Ind[curr_word], word2Ind[neighbor_word]] = 1\n",
        "            \n",
        "            elif i == len(sentences) - window_size:\n",
        "                for j in range(min(window_size, len(sentences))):\n",
        "                    neighbor_word = sentences[i-j-1]\n",
        "                    M[word2Ind[curr_word], word2Ind[neighbor_word]] = 1   \n",
        "                \n",
        "            else:\n",
        "                for j in range(min(window_size, len(sentences))):\n",
        "                    neighbor_word1 = sentences[i-j-1]\n",
        "                    neighbor_word2 = sentences[i+j+1]\n",
        "                    M[word2Ind[curr_word], word2Ind[neighbor_word1]] = 1\n",
        "                    M[word2Ind[curr_word], word2Ind[neighbor_word2]] = 1\n",
        "    # ------------------\n",
        "\n",
        "    return M, word2Ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9JXFIRJ4rcQ",
        "colab_type": "code",
        "outputId": "a17935c6-1961-4496-9e2c-5040e35f3bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\n",
        "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2Ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "word2Ind_ans = {'All': 0, \"All's\": 1, 'END': 2, 'START': 3, 'ends': 4, 'glitters': 5, 'gold': 6, \"isn't\": 7, 'that': 8, 'well': 9}\n",
        "\n",
        "# Test correct word2Ind\n",
        "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2Ind_ans.keys():\n",
        "    idx1 = word2Ind_ans[w1]\n",
        "    for w2 in word2Ind_ans.keys():\n",
        "        idx2 = word2Ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvIbB6ut4rbP",
        "colab_type": "code",
        "outputId": "5a42b277-3868-42f9-b899-6199022f4bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# X = sparse_random_matrix(100, 100, density=0.01, random_state=42)\n",
        "svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
        "X2 = svd.fit_transform(M_test)\n",
        "svd.fit(M_test)\n",
        "print(svd.explained_variance_ratio_)  \n",
        "print(svd.explained_variance_ratio_.sum())  \n",
        "print(svd.singular_values_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.07057732 0.27338415]\n",
            "0.34396147222110973\n",
            "[2.71298245 2.19352709]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "850gpP-k4rVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of corpus words, number of number of corpus words)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=10, random_state=42)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "\n",
        "    \n",
        "        # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpTKT_B4rTl",
        "colab_type": "code",
        "outputId": "cfc59136-d4bd-4df3-b0b0-6c60933aa8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\n",
        "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Truncated SVD over 10 words...\n",
            "Done.\n",
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfM8NgG4QP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_embeddings(M_reduced, word2Ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings\n",
        "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    X,Y = [], []\n",
        "    for word in words:\n",
        "        X.append(M_reduced[word2Ind[word], 0])\n",
        "        Y.append(M_reduced[word2Ind[word], 1])\n",
        "    X = np.array(X)\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(X, Y, marker='x', color='red')\n",
        "    for i, name in enumerate(words):\n",
        "        ax.annotate(name, (X[i], Y[i]))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # ------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odC2mhX35bQJ",
        "colab_type": "code",
        "outputId": "a538be1b-7304-4bc0-b886-ff9bc8e78a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below. \n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2Ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Outputted Plot:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfRDBGll5bZ5",
        "colab_type": "code",
        "outputId": "0ddc2f2c-c57b-4d18-9ac2-3080d72fba72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
        "plot_embeddings(M_normalized, word2Ind_co_occurrence, words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Truncated SVD over 8185 words...\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc7FwUU0zosX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "59ac99f3-6c03-4bec-93c1-1cf7c8ecb40d"
      },
      "source": [
        "# Download Google news vector\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-07 08:24:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.19.59\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.19.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: GoogleNews-vectors-negative300.bin.gz\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.6MB/s    in 99s     \n",
            "\n",
            "2020-06-07 08:26:30 (15.9 MB/s) - GoogleNews-vectors-negative300.bin.gz saved [1647046227/1647046227]\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGM-tj4-14Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCkoKI491yUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4fcffd49-e872-491f-b53f-ff0c232d4eaf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin     sample_data\n",
            "GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZmfiWu-0lGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-mNnO85bc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill this variable with the path to your downloaded and unzipped embeddings (`GoogleNews-vectors-negative300.bin` file).\n",
        "#\n",
        "# For Windows users place the `GoogleNews-vectors-negative300.bin` file in your conda environment's installation of gensim:\n",
        "# `envs/{conda_env_name}/lib/site-packages/gensim/test/test_data`\n",
        "# \n",
        "# For Mac/Linux users, you can place the `GoogleNews-vectors-negative300.bin` file anywhere on your machine.\n",
        "# \n",
        "\n",
        "embeddings_fp = \"/content/GoogleNews-vectors-negative300.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqTRCKlCzn1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mkeIxD05bmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word2vec(embeddings_fp=embeddings_fp):\n",
        "    \"\"\" Load Word2Vec Vectors\n",
        "        Param:\n",
        "            embeddings_fp (string) - path to .bin file of pretrained word vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 3 million embeddings, each lengh 300\n",
        "                This is the KeyedVectors format: https://radimrehurek.com/gensim/models/deprecated/keyedvectors.html\n",
        "    \"\"\"\n",
        "    embed_size = 300\n",
        "    print(\"Loading 3 million word vectors from file...\")\n",
        "    wv_from_bin = KeyedVectors.load_word2vec_format(datapath(embeddings_fp), binary=True)\n",
        "    vocab = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Loaded vocab size %i\" % len(vocab))\n",
        "    return wv_from_bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EeJvYgF5bk3",
        "colab_type": "code",
        "outputId": "91b270ad-50dc-48bf-fceb-c3e5d0409a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_word2vec()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 3 million word vectors from file...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded vocab size 3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxubDtlH5JJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_matrix_of_vectors(wv_from_bin):\n",
        "    \"\"\" Put the word2vec vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 3 million word2vec vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 300) containing the vectors\n",
        "            word2Ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    words = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
        "    word2Ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2Ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyjZWD9t5bji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "81cee046-2059-4308-ae22-9087bf5e01aa"
      },
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 300-Dimensinal Word Embeddings to k Dimensions\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------------------------------------\n",
        "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Putting 3000000 words into word2Ind and matrix M...\n",
            "Done.\n",
            "Running Truncated SVD over 3000000 words...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td1ZGuma5bho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "9478d875-9915-41fb-ef8e-ab8c87bceeb4"
      },
      "source": [
        "!git clone https://github.com/youngmihuang/cs224n_exercise.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cs224n_exercise'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Total 189 (delta 0), reused 0 (delta 0), pack-reused 189\u001b[K\n",
            "Receiving objects: 100% (189/189), 52.88 MiB | 10.44 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "Checking out files: 100% (114/114), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9_DInmm6HLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "8d50f2cf-7c99-456b-bf62-b3fb9794774a"
      },
      "source": [
        "! ls cs224n_exercise/*"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cs224n_exercise/README.md\n",
            "\n",
            "cs224n_exercise/assignment1:\n",
            "exploring_word_vectors.ipynb  exploring_word_vectors.pdf  README.txt\n",
            "\n",
            "cs224n_exercise/assignment2:\n",
            "a2.pdf\t\t       env.yml\t\trun.py\tutils\t     word_vectors.png\n",
            "collect_submission.sh  get_datasets.sh\tsgd.py\tword2vec.py\n",
            "\n",
            "cs224n_exercise/assignment3:\n",
            "a3.pdf\t\t       data\t\tparser_transitions.py  utils\n",
            "collect_submission.sh  parser_model.py\trun.py\n",
            "\n",
            "cs224n_exercise/assignment4:\n",
            "a4.pdf\t\t       local_env.yml\t    run.sh\n",
            "collect_submission.sh  model_embeddings.py  sanity_check_en_es_data\n",
            "en_es_data\t       nmt_model.py\t    sanity_check.py\n",
            "gpu_requirements.txt   README.md\t    utils.py\n",
            "__init__.py\t       run.py\t\t    vocab.py\n",
            "\n",
            "cs224n_exercise/assignment5:\n",
            "a5.pdf\n",
            "\n",
            "cs224n_exercise/images:\n",
            "02_result_naiveGrad.png  02_result_negSamplingGrad.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gz2ZIj56HTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64f98c3e-ff8e-4e75-b45d-3778ffea7f76"
      },
      "source": [
        "!cat cs224n_exercise/assignment3/parser_model.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "CS224N 2018-19: Homework 3\n",
            "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
            "Editor: Youngmi Huang\n",
            "\"\"\"\n",
            "import pickle\n",
            "import os\n",
            "import time\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "\n",
            "class ParserModel(nn.Module):\n",
            "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
            "    The ParserModel will predict which transition should be applied to a\n",
            "    given partial parse configuration.\n",
            "\n",
            "    PyTorch Notes:\n",
            "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
            "            are a subclass of this \"nn.Module\".\n",
            "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
            "            (embedding layers, linear layers, dropout layers, etc.).\n",
            "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
            "            when you write \"m = ParserModel()\".\n",
            "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
            "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
            "            in other ParserModel methods.\n",
            "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
            "    \"\"\"\n",
            "    def __init__(self, embeddings, n_features=36,\n",
            "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
            "        \"\"\" Initialize the parser model.\n",
            "\n",
            "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
            "        @param n_features (int): number of input features\n",
            "        @param hidden_size (int): number of hidden units\n",
            "        @param n_classes (int): number of output classes\n",
            "        @param dropout_prob (float): dropout probability\n",
            "        \"\"\"\n",
            "        super(ParserModel, self).__init__()\n",
            "        self.n_features = n_features\n",
            "        self.n_classes = n_classes\n",
            "        self.dropout_prob = dropout_prob\n",
            "        self.embed_size = embeddings.shape[1]\n",
            "        self.hidden_size = hidden_size\n",
            "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
            "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
            "\n",
            "\n",
            "        ### YOUR CODE HERE (~5 Lines)\n",
            "        ### TODO:\n",
            "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
            "        ###         with the `nn.init.xavier_uniform_` function (without any gain)\n",
            "        ###     2) Construct `self.dropout` layer.\n",
            "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
            "        ###         with the `nn.init.xavier_uniform_` function (without any gain)\n",
            "        ###\n",
            "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
            "        ###         It has been shown empirically, that this provides better initial weights\n",
            "        ###         for training networks than random uniform initialization.\n",
            "        ###         For more details checkout this great blogpost:\n",
            "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
            "        ### Hints:\n",
            "        ###     - After you create a linear layer you can access the weight\n",
            "        ###       matrix via:\n",
            "        ###         linear_layer.weight\n",
            "        ###\n",
            "        ### Please see the following docs for support:\n",
            "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
            "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
            "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
            "\n",
            "        self.embed_shapes = embeddings.shape\n",
            "        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, self.hidden_size)\n",
            "        nn.init.xavier_uniform_(self.embed_to_hidden.weight)\n",
            "        self.dropout = nn.Dropout(p=dropout_prob)\n",
            "        self.hidden_to_logits = nn.Linear(self.hidden_size, n_classes)\n",
            "        nn.init.xavier_uniform_(self.hidden_to_logits.weight)\n",
            "        ### END YOUR CODE\n",
            "\n",
            "    def embedding_lookup(self, t):\n",
            "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
            "            to embedding vectors.\n",
            "\n",
            "            PyTorch Notes:\n",
            "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
            "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
            "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
            "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
            "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
            "\n",
            "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
            "\n",
            "            @return x (Tensor): tensor of embeddings for words represented in t\n",
            "                                (batch_size, n_features * embed_size)\n",
            "        \"\"\"\n",
            "        ### YOUR CODE HERE (~1-3 Lines)\n",
            "        ### TODO:\n",
            "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
            "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
            "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
            "        ###\n",
            "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
            "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
            "        ###\n",
            "        ###  Please see the following docs for support:\n",
            "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
            "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
            "\n",
            "        t_embed = self.pretrained_embeddings(t)\n",
            "        shapes = t_embed.size()\n",
            "        x = t_embed.view(shapes[0], -1)\n",
            "        ### END YOUR CODE\n",
            "        return x\n",
            "\n",
            "\n",
            "    def forward(self, t):\n",
            "        \"\"\" Run the model forward.\n",
            "\n",
            "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
            "\n",
            "            PyTorch Notes:\n",
            "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
            "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
            "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
            "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
            "                        model = ParserModel()\n",
            "                        output = model(t) # this calls the forward function\n",
            "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
            "\n",
            "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
            "\n",
            "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
            "                                 without applying softmax (batch_size, n_classes)\n",
            "        \"\"\"\n",
            "\n",
            "        ###  YOUR CODE HERE (~3-5 lines)\n",
            "        ### TODO:\n",
            "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
            "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
            "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
            "        ###     4) Apply dropout layer to the output of step 3.\n",
            "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
            "        ###\n",
            "        ### Note: We do not apply the softmax to the logits here, because\n",
            "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
            "        ###\n",
            "        ### Please see the following docs for support:\n",
            "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
            "        \n",
            "        x = self.embedding_lookup(t)\n",
            "        x = self.embed_to_hidden(x)\n",
            "        x = nn.functional.relu(x)\n",
            "        x = self.dropout(x)\n",
            "        logits = self.hidden_to_logits(x)\n",
            "        ### END YOUR CODE\n",
            "        return logits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0TRDZ406HZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
        "Editor: Youngmi Huang\n",
        "\"\"\"\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ParserModel(nn.Module):\n",
        "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
        "    The ParserModel will predict which transition should be applied to a\n",
        "    given partial parse configuration.\n",
        "\n",
        "    PyTorch Notes:\n",
        "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
        "            are a subclass of this \"nn.Module\".\n",
        "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
        "            (embedding layers, linear layers, dropout layers, etc.).\n",
        "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
        "            when you write \"m = ParserModel()\".\n",
        "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
        "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
        "            in other ParserModel methods.\n",
        "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, n_features=36,\n",
        "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
        "        \"\"\" Initialize the parser model.\n",
        "\n",
        "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
        "        @param n_features (int): number of input features\n",
        "        @param hidden_size (int): number of hidden units\n",
        "        @param n_classes (int): number of output classes\n",
        "        @param dropout_prob (float): dropout probability\n",
        "        \"\"\"\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "\n",
        "        ### YOUR CODE HERE (~5 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
        "        ###         with the `nn.init.xavier_uniform_` function (without any gain)\n",
        "        ###     2) Construct `self.dropout` layer.\n",
        "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
        "        ###         with the `nn.init.xavier_uniform_` function (without any gain)\n",
        "        ###\n",
        "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
        "        ###         It has been shown empirically, that this provides better initial weights\n",
        "        ###         for training networks than random uniform initialization.\n",
        "        ###         For more details checkout this great blogpost:\n",
        "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
        "        ### Hints:\n",
        "        ###     - After you create a linear layer you can access the weight\n",
        "        ###       matrix via:\n",
        "        ###         linear_layer.weight\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
        "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "\n",
        "        self.embed_shapes = embeddings.shape\n",
        "        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, self.hidden_size)\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden.weight)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(self.hidden_size, n_classes)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logits.weight)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
        "            to embedding vectors.\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
        "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
        "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
        "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
        "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
        "\n",
        "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "            @return x (Tensor): tensor of embeddings for words represented in t\n",
        "                                (batch_size, n_features * embed_size)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~1-3 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
        "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
        "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
        "        ###\n",
        "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
        "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
        "        ###\n",
        "        ###  Please see the following docs for support:\n",
        "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "\n",
        "        t_embed = self.pretrained_embeddings(t)\n",
        "        shapes = t_embed.size()\n",
        "        x = t_embed.view(shapes[0], -1)\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\" Run the model forward.\n",
        "\n",
        "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
        "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
        "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
        "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
        "                        model = ParserModel()\n",
        "                        output = model(t) # this calls the forward function\n",
        "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
        "\n",
        "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
        "                                 without applying softmax (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        ###  YOUR CODE HERE (~3-5 lines)\n",
        "        ### TODO:\n",
        "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
        "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
        "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
        "        ###     4) Apply dropout layer to the output of step 3.\n",
        "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
        "        ###\n",
        "        ### Note: We do not apply the softmax to the logits here, because\n",
        "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
        "        \n",
        "        x = self.embedding_lookup(t)\n",
        "        x = self.embed_to_hidden(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.hidden_to_logits(x)\n",
        "        ### END YOUR CODE\n",
        "        return logits\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7JYyCzj4QTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bGzNJqC4Qa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}